{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Reference <http://neuralnetworksanddeeplearning.com/chap3.html#regularization>\n",
    "\n",
    "Regularization is a techniques which can reduce overfitting\n",
    "\n",
    "What is overfitting and why regularization works <http://neuralnetworksanddeeplearning.com/chap3.html#why_does_regularization_help_reduce_overfitting>\n",
    "\n",
    "Suppose our network mostly has small weights, as will tend to happen in a regularized network. The smallness of the weights means that the behavior of the network won't change too much if we change a few random inputs here and there. That makes it difficult for a regularized network to learn the effects of local noise in the data. \n",
    "\n",
    "A network with large weights may change its behaviour quite a bit in response to small changes in the input. Unregularized network can use large weights to learn a complex model that carries a lot of information about the noise in the training data.\n",
    "\n",
    "A regularized networks are constrained to build relatively simple models based on patterns seen often in the training data, and are resistant to learning peculiarities of the noise in the training data. This will force our networks to do real learning about the phenomenon at hand, and to generalize better from what they learn.\n",
    "\n",
    "No-one has yet developed an entirely convincing theoretical explanation for why regularization helps networks generalize\n",
    "\n",
    "our networks already generalize better than one might a priori expect. A network with 100 hidden neurons has nearly 80,000 parameters. We have only 50,000 images in our training data. It's like trying to fit an 80,000th degree polynomial to 50,000 data points. By all rights, our network should overfit terribly. And yet, as we saw earlier, such a network actually does a pretty good job generalizing. Why is that the case? It's not well understood. It has been conjectured In Gradient-Based Learning Applied to Document Recognition, by Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner (1998). that \"the dynamics of gradient descent learning in multilayer nets has a self-regularization effect\"\n",
    "\n",
    "\n",
    "## L2 regularization\n",
    "\n",
    "The idea of _weight decay_ or _L2 regularization_ is to add _regularization term_ to the cost function. Intuitively, the effect of regularization is to make it so the network prefers to learn small weights, all other things being equal.\n",
    "\n",
    "$\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{2n}\n",
    "\\sum_w w^2,\n",
    "\\tag{87}\\end{eqnarray}$\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\frac{\\partial C}{\\partial w} & = & \\frac{\\partial C_0}{\\partial w} + \n",
    "  \\frac{\\lambda}{n} w \\tag{88}\\\\ \n",
    "  \\frac{\\partial C}{\\partial b} & = & \\frac{\\partial C_0}{\\partial b}.\n",
    "\\tag{89}\\end{eqnarray}$\n",
    "\n",
    "Use back propagation to compute partial derivatives \n",
    "\n",
    "$\\begin{eqnarray}\n",
    "b & \\rightarrow & b -\\eta \\frac{\\partial C_0}{\\partial b}.\n",
    "\\tag{90}\\end{eqnarray}$\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  w & \\rightarrow & w-\\eta \\frac{\\partial C_0}{\\partial\n",
    "    w}-\\frac{\\eta \\lambda}{n} w \\tag{91}\\\\ \n",
    "  & = & \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial\n",
    "    C_0}{\\partial w}. \n",
    "\\tag{92}\\end{eqnarray}$\n",
    "\n",
    "This is exactly the same as the usual gradient descent learning rule, except we first rescale the weight w by a factor (weight decay) $1-\\frac{\\eta\\lambda}{n}$ \n",
    "\n",
    "Stochastic gradient descent works in the same way. \n",
    "\n",
    "\n",
    "### Cross-entropy regularized:\n",
    "\n",
    "$\\begin{eqnarray} C = -\\frac{1}{n} \\sum_{xj} \\left[ y_j \\ln a^L_j+(1-y_j) \\ln\n",
    "(1-a^L_j)\\right] + \\frac{\\lambda}{2n} \\sum_w w^2.\n",
    "\\tag{85}\\end{eqnarray}$\n",
    "\n",
    "### Quadratic cost regularized:\n",
    "\n",
    "$C = \\frac{1}{2n} \\sum_x \\|y-a^L\\|^2 +\n",
    "  \\frac{\\lambda}{2n} \\sum_w w^2\\tag{86}$\n",
    "\n",
    "\n",
    "## L1 regularization\n",
    "\n",
    "<http://neuralnetworksanddeeplearning.com/chap3.html#other_techniques_for_regularization>\n",
    "\n",
    "In this approach we modify the unregularized cost function by adding the sum of the absolute values of the weights. This is similar to L2 regularization, penalizing large weights, and tending to make the network prefer small weights. In L1 regularization, the weights shrink by a constant amount toward 0. In L2 regularization, the weights shrink by an amount which is proportional to w.\n",
    "$\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{n} \\sum_w |w|.\n",
    "\\tag{95}\\end{eqnarray}$\n",
    "\n",
    "## Dropout\n",
    "\n",
    "With dropout, we start by randomly (and temporarily) deleting half the hidden neurons in the network, while leaving the input and output neurons untouched. We forward-propagate the input x through the modified network, and then backpropagate the result, also through the modified network. After doing this over a mini-batch of examples, we update the appropriate weights and biases. We then repeat the process, first restoring the dropout neurons, then choosing a new random subset of hidden neurons to delete, estimating the gradient for a different mini-batch, and updating the weights and biases in the network. \"This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\" (from <https://arxiv.org/pdf/1207.0580.pdf>)\n",
    "\n",
    "<http://neuralnetworksanddeeplearning.com/chap3.html#other_techniques_for_regularization>\n",
    "\n",
    "## Artificially expanding the training data\n",
    "\n",
    "The classification accuracies improve considerably as we use more training data (see original book)\n",
    "\n",
    "Obtaining more training data is a great idea. Unfortunately, it can be expensive, and so is not always possible in practice.\n",
    "\n",
    "Suppose, for example, that we take an MNIST training image of a five, and rotate it by a small amount, let's say 15 degrees. It's still recognizably the same digit. And yet at the pixel level it's quite different to any image currently in the MNIST training data. \n",
    "\n",
    "We can expand our training data by making many small rotations of all the MNIST training images, and then using the expanded training data to improve our network's performance.\n",
    "\n",
    "This idea is very powerful and has been widely used. Let's look at some of the results from a paper: Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis, by Patrice Simard, Dave Steinkraus, and John Platt (2003). which applied several variations of the idea to MNIST. <https://ieeexplore.ieee.org/document/1227801/>\n",
    "\n",
    "Variations on this idea can be used to improve performance on many learning tasks, not just handwriting recognition. The general principle is to expand the training data by applying operations that reflect real-world variation. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 122856.39730641841\n",
      "Accuracy on training data: 47245 / 50000\n",
      "Cost on evaluation data: 122856.41263689839\n",
      "Accuracy on evaluation data: 9428 / 10000\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 76528.48556196361\n",
      "Accuracy on training data: 47879 / 50000\n",
      "Cost on evaluation data: 76528.51504255563\n",
      "Accuracy on evaluation data: 9531 / 10000\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 48659.80088618216\n",
      "Accuracy on training data: 48413 / 50000\n",
      "Cost on evaluation data: 48659.8423165974\n",
      "Accuracy on evaluation data: 9623 / 10000\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 31955.063780332912\n",
      "Accuracy on training data: 48750 / 50000\n",
      "Cost on evaluation data: 31955.110152876627\n",
      "Accuracy on evaluation data: 9665 / 10000\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 21956.777308635923\n",
      "Accuracy on training data: 48734 / 50000\n",
      "Cost on evaluation data: 21956.83116751666\n",
      "Accuracy on evaluation data: 9671 / 10000\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 16052.33950766568\n",
      "Accuracy on training data: 48793 / 50000\n",
      "Cost on evaluation data: 16052.394144022212\n",
      "Accuracy on evaluation data: 9651 / 10000\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 12543.111070164623\n",
      "Accuracy on training data: 49068 / 50000\n",
      "Cost on evaluation data: 12543.163478184908\n",
      "Accuracy on evaluation data: 9730 / 10000\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 10439.914111026828\n",
      "Accuracy on training data: 49102 / 50000\n",
      "Cost on evaluation data: 10439.973694202938\n",
      "Accuracy on evaluation data: 9712 / 10000\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 9156.947251957947\n",
      "Accuracy on training data: 48924 / 50000\n",
      "Cost on evaluation data: 9157.002393223975\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 8440.551216794132\n",
      "Accuracy on training data: 48972 / 50000\n",
      "Cost on evaluation data: 8440.604796855961\n",
      "Accuracy on evaluation data: 9710 / 10000\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 8000.657423236137\n",
      "Accuracy on training data: 48970 / 50000\n",
      "Cost on evaluation data: 8000.718967385341\n",
      "Accuracy on evaluation data: 9693 / 10000\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 7728.390734786435\n",
      "Accuracy on training data: 48658 / 50000\n",
      "Cost on evaluation data: 7728.446398839922\n",
      "Accuracy on evaluation data: 9641 / 10000\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 7545.710500564497\n",
      "Accuracy on training data: 49285 / 50000\n",
      "Cost on evaluation data: 7545.769622922249\n",
      "Accuracy on evaluation data: 9751 / 10000\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 7451.168966228948\n",
      "Accuracy on training data: 49207 / 50000\n",
      "Cost on evaluation data: 7451.234488388631\n",
      "Accuracy on evaluation data: 9717 / 10000\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 7383.073922530803\n",
      "Accuracy on training data: 49240 / 50000\n",
      "Cost on evaluation data: 7383.131620146465\n",
      "Accuracy on evaluation data: 9736 / 10000\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 7353.506869370283\n",
      "Accuracy on training data: 49125 / 50000\n",
      "Cost on evaluation data: 7353.5613496092765\n",
      "Accuracy on evaluation data: 9729 / 10000\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 7317.988063071357\n",
      "Accuracy on training data: 49131 / 50000\n",
      "Cost on evaluation data: 7318.048456169989\n",
      "Accuracy on evaluation data: 9723 / 10000\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 7316.077473318902\n",
      "Accuracy on training data: 49349 / 50000\n",
      "Cost on evaluation data: 7316.13880530304\n",
      "Accuracy on evaluation data: 9754 / 10000\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 7294.721740898603\n",
      "Accuracy on training data: 49075 / 50000\n",
      "Cost on evaluation data: 7294.785851583118\n",
      "Accuracy on evaluation data: 9694 / 10000\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 7292.561369959138\n",
      "Accuracy on training data: 49222 / 50000\n",
      "Cost on evaluation data: 7292.624172321703\n",
      "Accuracy on evaluation data: 9732 / 10000\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 7269.841663426315\n",
      "Accuracy on training data: 49325 / 50000\n",
      "Cost on evaluation data: 7269.904325086249\n",
      "Accuracy on evaluation data: 9735 / 10000\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 7275.710908279746\n",
      "Accuracy on training data: 49389 / 50000\n",
      "Cost on evaluation data: 7275.775935198385\n",
      "Accuracy on evaluation data: 9745 / 10000\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 7262.744656683118\n",
      "Accuracy on training data: 49360 / 50000\n",
      "Cost on evaluation data: 7262.809938376528\n",
      "Accuracy on evaluation data: 9744 / 10000\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 7250.638801949688\n",
      "Accuracy on training data: 49105 / 50000\n",
      "Cost on evaluation data: 7250.7066678418\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 7261.266828682565\n",
      "Accuracy on training data: 49321 / 50000\n",
      "Cost on evaluation data: 7261.324092362781\n",
      "Accuracy on evaluation data: 9744 / 10000\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 7293.428889918885\n",
      "Accuracy on training data: 49332 / 50000\n",
      "Cost on evaluation data: 7293.483343448961\n",
      "Accuracy on evaluation data: 9761 / 10000\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 7273.423868337474\n",
      "Accuracy on training data: 49166 / 50000\n",
      "Cost on evaluation data: 7273.486928160647\n",
      "Accuracy on evaluation data: 9726 / 10000\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 7271.945159713751\n",
      "Accuracy on training data: 49458 / 50000\n",
      "Cost on evaluation data: 7272.006694647641\n",
      "Accuracy on evaluation data: 9762 / 10000\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 7241.984668056722\n",
      "Accuracy on training data: 49313 / 50000\n",
      "Cost on evaluation data: 7242.041852565861\n",
      "Accuracy on evaluation data: 9743 / 10000\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 7225.423134130623\n",
      "Accuracy on training data: 49393 / 50000\n",
      "Cost on evaluation data: 7225.485794565235\n",
      "Accuracy on evaluation data: 9763 / 10000\n"
     ]
    }
   ],
   "source": [
    "# Reset and load python functions\n",
    "%reset\n",
    "%run python/download_data.py\n",
    "%run python/mnist_loader.py\n",
    "%run python/network2.py\n",
    "%run python/overfitting.py\n",
    "\n",
    "# Experiment 3\n",
    "\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "net = Network([784, 100, 10], cost=CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "# Hyper-parameters:\n",
    "num_epochs_3 = 30\n",
    "batch_size_3 = 10\n",
    "learning_3 = 0.5\n",
    "lmbda_3 = 5.0\n",
    "net.SGD(\n",
    "    training_data, num_epochs_3, batch_size_3, learning_3,\n",
    "    evaluation_data=validation_data, lmbda = lmbda_3,\n",
    "    monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 8945 / 10000\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9171 / 10000\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9294 / 10000\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9376 / 10000\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9425 / 10000\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9470 / 10000\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9507 / 10000\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9513 / 10000\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9553 / 10000\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9583 / 10000\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9593 / 10000\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9614 / 10000\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9638 / 10000\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9639 / 10000\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9652 / 10000\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9670 / 10000\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9675 / 10000\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9670 / 10000\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9693 / 10000\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9700 / 10000\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9709 / 10000\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9713 / 10000\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9711 / 10000\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9731 / 10000\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9731 / 10000\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9734 / 10000\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9739 / 10000\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9740 / 10000\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9739 / 10000\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9744 / 10000\n",
      "Epoch 30 training complete\n",
      "Accuracy on evaluation data: 9736 / 10000\n",
      "Epoch 31 training complete\n",
      "Accuracy on evaluation data: 9749 / 10000\n",
      "Epoch 32 training complete\n",
      "Accuracy on evaluation data: 9750 / 10000\n",
      "Epoch 33 training complete\n",
      "Accuracy on evaluation data: 9748 / 10000\n",
      "Epoch 34 training complete\n",
      "Accuracy on evaluation data: 9763 / 10000\n",
      "Epoch 35 training complete\n",
      "Accuracy on evaluation data: 9760 / 10000\n",
      "Epoch 36 training complete\n",
      "Accuracy on evaluation data: 9759 / 10000\n",
      "Epoch 37 training complete\n",
      "Accuracy on evaluation data: 9765 / 10000\n",
      "Epoch 38 training complete\n",
      "Accuracy on evaluation data: 9764 / 10000\n",
      "Epoch 39 training complete\n",
      "Accuracy on evaluation data: 9756 / 10000\n",
      "Epoch 40 training complete\n",
      "Accuracy on evaluation data: 9758 / 10000\n",
      "Epoch 41 training complete\n",
      "Accuracy on evaluation data: 9757 / 10000\n",
      "Epoch 42 training complete\n",
      "Accuracy on evaluation data: 9769 / 10000\n",
      "Epoch 43 training complete\n",
      "Accuracy on evaluation data: 9779 / 10000\n",
      "Epoch 44 training complete\n",
      "Accuracy on evaluation data: 9776 / 10000\n",
      "Epoch 45 training complete\n",
      "Accuracy on evaluation data: 9772 / 10000\n",
      "Epoch 46 training complete\n",
      "Accuracy on evaluation data: 9786 / 10000\n",
      "Epoch 47 training complete\n",
      "Accuracy on evaluation data: 9779 / 10000\n",
      "Epoch 48 training complete\n",
      "Accuracy on evaluation data: 9786 / 10000\n",
      "Epoch 49 training complete\n",
      "Accuracy on evaluation data: 9775 / 10000\n",
      "Epoch 50 training complete\n",
      "Accuracy on evaluation data: 9780 / 10000\n",
      "Epoch 51 training complete\n",
      "Accuracy on evaluation data: 9781 / 10000\n",
      "Epoch 52 training complete\n",
      "Accuracy on evaluation data: 9760 / 10000\n",
      "Epoch 53 training complete\n",
      "Accuracy on evaluation data: 9790 / 10000\n",
      "Epoch 54 training complete\n",
      "Accuracy on evaluation data: 9786 / 10000\n",
      "Epoch 55 training complete\n",
      "Accuracy on evaluation data: 9800 / 10000\n",
      "Epoch 56 training complete\n",
      "Accuracy on evaluation data: 9789 / 10000\n",
      "Epoch 57 training complete\n",
      "Accuracy on evaluation data: 9792 / 10000\n",
      "Epoch 58 training complete\n",
      "Accuracy on evaluation data: 9786 / 10000\n",
      "Epoch 59 training complete\n",
      "Accuracy on evaluation data: 9795 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [8945,\n",
       "  9171,\n",
       "  9294,\n",
       "  9376,\n",
       "  9425,\n",
       "  9470,\n",
       "  9507,\n",
       "  9513,\n",
       "  9553,\n",
       "  9583,\n",
       "  9593,\n",
       "  9614,\n",
       "  9638,\n",
       "  9639,\n",
       "  9652,\n",
       "  9670,\n",
       "  9675,\n",
       "  9670,\n",
       "  9693,\n",
       "  9700,\n",
       "  9709,\n",
       "  9713,\n",
       "  9711,\n",
       "  9731,\n",
       "  9731,\n",
       "  9734,\n",
       "  9739,\n",
       "  9740,\n",
       "  9739,\n",
       "  9744,\n",
       "  9736,\n",
       "  9749,\n",
       "  9750,\n",
       "  9748,\n",
       "  9763,\n",
       "  9760,\n",
       "  9759,\n",
       "  9765,\n",
       "  9764,\n",
       "  9756,\n",
       "  9758,\n",
       "  9757,\n",
       "  9769,\n",
       "  9779,\n",
       "  9776,\n",
       "  9772,\n",
       "  9786,\n",
       "  9779,\n",
       "  9786,\n",
       "  9775,\n",
       "  9780,\n",
       "  9781,\n",
       "  9760,\n",
       "  9790,\n",
       "  9786,\n",
       "  9800,\n",
       "  9789,\n",
       "  9792,\n",
       "  9786,\n",
       "  9795],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "\n",
    "# Experiment 4 - Some tuning - 60 epochs at Î·=0.1 and Î»=5.0\n",
    "\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "net = Network([784, 100, 10], cost=CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "# Hyper-parameters:\n",
    "num_epochs_4 = 60\n",
    "batch_size_4 = 10\n",
    "learning_4 = 0.1\n",
    "lmbda_4 = 5.0\n",
    "net.SGD(\n",
    "    training_data, num_epochs_4, batch_size_4, learning_4,\n",
    "    evaluation_data=validation_data, lmbda = lmbda_4,\n",
    "    monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
