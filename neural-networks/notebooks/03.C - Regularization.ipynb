{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Reference <http://neuralnetworksanddeeplearning.com/chap3.html#regularization>\n",
    "\n",
    "Regularization is a techniques which can reduce overfitting\n",
    "\n",
    "What is overfitting and why regularization works <http://neuralnetworksanddeeplearning.com/chap3.html#why_does_regularization_help_reduce_overfitting>\n",
    "\n",
    "Suppose our network mostly has small weights, as will tend to happen in a regularized network. The smallness of the weights means that the behavior of the network won't change too much if we change a few random inputs here and there. That makes it difficult for a regularized network to learn the effects of local noise in the data. \n",
    "\n",
    "A network with large weights may change its behaviour quite a bit in response to small changes in the input. Unregularized network can use large weights to learn a complex model that carries a lot of information about the noise in the training data.\n",
    "\n",
    "A regularized networks are constrained to build relatively simple models based on patterns seen often in the training data, and are resistant to learning peculiarities of the noise in the training data. This will force our networks to do real learning about the phenomenon at hand, and to generalize better from what they learn.\n",
    "\n",
    "No-one has yet developed an entirely convincing theoretical explanation for why regularization helps networks generalize\n",
    "\n",
    "our networks already generalize better than one might a priori expect. A network with 100 hidden neurons has nearly 80,000 parameters. We have only 50,000 images in our training data. It's like trying to fit an 80,000th degree polynomial to 50,000 data points. By all rights, our network should overfit terribly. And yet, as we saw earlier, such a network actually does a pretty good job generalizing. Why is that the case? It's not well understood. It has been conjectured In Gradient-Based Learning Applied to Document Recognition, by Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner (1998). that \"the dynamics of gradient descent learning in multilayer nets has a self-regularization effect\"\n",
    "\n",
    "\n",
    "## L2 regularization\n",
    "\n",
    "The idea of _weight decay_ or _L2 regularization_ is to add _regularization term_ to the cost function\n",
    "\n",
    "$\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{2n}\n",
    "\\sum_w w^2,\n",
    "\\tag{87}\\end{eqnarray}$\n",
    "\n",
    "Intuitively, the effect of regularization is to make it so the network prefers to learn small weights, all other things being equal.\n",
    "\n",
    "Cross-entropy regularized:\n",
    "\n",
    "$\\begin{eqnarray} C = -\\frac{1}{n} \\sum_{xj} \\left[ y_j \\ln a^L_j+(1-y_j) \\ln\n",
    "(1-a^L_j)\\right] + \\frac{\\lambda}{2n} \\sum_w w^2.\n",
    "\\tag{85}\\end{eqnarray}$\n",
    "\n",
    "Quadratic cost regularized:\n",
    "\n",
    "$C = \\frac{1}{2n} \\sum_x \\|y-a^L\\|^2 +\n",
    "  \\frac{\\lambda}{2n} \\sum_w w^2\\tag{86}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
