{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function\n",
    "\n",
    "<http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent>\n",
    "\n",
    "What we'd like is an algorithm which lets us find weights and biases so that the output from the network approximates y(x) for all training inputs x\n",
    "\n",
    "Our goal in training a neural network is to find weights and biases which minimize the quadratic cost function  C(w,b)\n",
    "\n",
    "Define a cost function (or loss function)\n",
    "\n",
    "C is mean squared error (MSR) function or quadratic cost function\n",
    "\n",
    "$C(w,b) \\equiv\\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2.$\n",
    "\n",
    "Where\n",
    "\n",
    "- $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$  is the desired output from the network\n",
    "- w = collection of all weights\n",
    "- b = all the biases\n",
    "- n = total number of training inputs\n",
    "- a =  vector of outputs from the network when x is input\n",
    "- x = is all training inputs\n",
    "\n",
    "C(w,b)≈0 when y(x) is approximately equal to the output a\n",
    "\n",
    "\n",
    "Vector length <https://en.wikipedia.org/wiki/Euclidean_vector#Length>\n",
    "\n",
    "The aim of a training algorithm is to minimize the cost C(w,b) as a function of the weights and biases\n",
    "\n",
    "From chapter 3 <http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function>\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\frac{\\partial C}{\\partial w} & = & (a-y)\\sigma'(z) x = a \\sigma'(z) \\tag{55}\\\\\n",
    "  \\frac{\\partial C}{\\partial b} & = & (a-y)\\sigma'(z) = a \\sigma'(z),\n",
    "\\tag{56}\\end{eqnarray}$\n",
    "\n",
    "Since $\\sigma'(z)$ is present in (55) and (56) mean squared error is sensible to \"learn slowly\" effect\n",
    "\n",
    "Cross-entropy cost function solve this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "Credits <http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent>\n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "$C(v1, v2, ...)$\n",
    "\n",
    "$  \\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 + \\frac{\\partial C}{\\partial v_2} \\Delta v_2$\n",
    "\n",
    "$\\Delta v \\equiv (\\Delta v_1, \\Delta v_2)^T$\n",
    "\n",
    "$  \\nabla C \\equiv \\left( \\frac{\\partial C}{\\partial v_1},\\frac{\\partial C}{\\partial v_2} \\right)^T$\n",
    "\n",
    "$ \\Delta C \\approx \\nabla C \\cdot \\Delta v$\n",
    "\n",
    "$\\nabla C$ is the gradient vector and it relates changes in v to changes in C\n",
    "\n",
    "$ \\Delta v = -\\eta \\nabla C$\n",
    "\n",
    "$\\eta$ = learning rate (small positive parameter)\n",
    "\n",
    "$\\Delta C \\approx -\\eta \\nabla C \\cdot \\nabla C = -\\eta \\|\\nabla C\\|^2$\n",
    "\n",
    "Because $\\| \\nabla C\\|^2 \\geq 0 \\implies \\Delta C \\leq 0$ \n",
    "\n",
    "$v \\rightarrow v' = v -\\eta \\nabla C$\n",
    "\n",
    "Summing up, the way the gradient descent algorithm works is to repeatedly compute the gradient ∇C, and then to move in the opposite direction, \"falling down\" the slope of the valley.\n",
    "\n",
    "Python implementations:\n",
    "\n",
    "- <https://martinapugliese.github.io/dissecting-the-gradient-descent-method/>\n",
    "- <https://github.com/dtnewman/gradient_descent/blob/master/stochastic_gradient_descent.ipynb>\n",
    "- <https://github.com/crsmithdev/notebooks>\n",
    "\n",
    "# Stochastic gradient descent\n",
    "\n",
    "....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression \n",
    "\n",
    "Use gradient descent to find theminimum of the weight function of linera regression\n",
    "\n",
    "- math background <http://www.deeplearningbook.org/contents/ml.html> page 107\n",
    "- <https://www.kaggle.com/tentotheminus9/linear-regression-from-scratch-gradient-descent>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
