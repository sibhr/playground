{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "<http://neuralnetworksanddeeplearning.com/chap2.html>\n",
    "\n",
    "The goal of backpropagation is to compute the partial derivatives ∂C/∂w and ∂C/∂b of the cost function  C\n",
    "\n",
    "OR\n",
    "\n",
    " the backpropagation algorithm is a clever way of keeping track of small perturbations to the weights (and biases) as they propagate through the network, reach the output, and then affect the cost <http://neuralnetworksanddeeplearning.com/chap2.html#backpropagation_the_big_picture>\n",
    "\n",
    "## Notations\n",
    "\n",
    "![img](http://neuralnetworksanddeeplearning.com/images/tikz17.png)\n",
    "\n",
    "$w^l_{jk}$ = weight for the connection from the k th neuron in the (l−1)th layer to the j th neuron in the lth layer\n",
    "\n",
    "$b^l_j$ = bias of the  $j^{\\rm th}$ neuron in the $l^{\\rm th}$ layer\n",
    "\n",
    "$a^l_j$ = activation of of the  $j^{\\rm th}$ neuron in the $l^{\\rm th}$ layer\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "Assumption 1: the cost function can be written as an average over cost functions Cx for individual training examples, x\n",
    "\n",
    "$C = \\frac{1}{2n} \\sum_x \\|y(x)-a^L(x)\\|^2 = \\frac{1}{n} \\sum_x C_x$ where $C_x = \\frac{1}{2} \\|y-a^L \\|^2$\n",
    "\n",
    "\n",
    "Assumption 2: the cost can be written as a function of the outputs from the neural network \n",
    "\n",
    "$C = \\frac{1}{2} \\|y-a^L\\|^2 = \\frac{1}{2} \\sum_j (y_j-a^L_j)^2$\n",
    "\n",
    "Detailed discussion <http://neuralnetworksanddeeplearning.com/chap2.html#the_two_assumptions_we_need_about_the_cost_function>\n",
    "\n",
    "## Equations\n",
    "\n",
    "$\\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j}$ is the error of neuron j in layer l\n",
    "\n",
    "Backpropagation gives a way of computing $\\delta^l_j$ for every layer, and then relating those errors to the quantities of real interest, $\\partial C / \\partial w^l_{jk}$ and  $\\partial C / \\partial b^l_{j}$ \n",
    "\n",
    "### Equation 1:  error in the output layer $\\delta^L$\n",
    "\n",
    "$\\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j)\\tag{BP1}$\n",
    "\n",
    "Matrix form: \n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\delta^L = \\nabla_a C \\odot \\sigma'(z^L)\n",
    "\\tag{BP1a}\\end{eqnarray}$\n",
    "\n",
    "$\\nabla_a C$  is defined to be a vector whose components are the partial derivatives over j\n",
    "\n",
    "Quadratic cost function $C = \\frac{1}{2} \\sum_j(y_j-a^L_j)^2$ implies $\\partial C / \\partial a^L_j = (a_j^L-y_j)$\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\delta^L = (a^L-y) \\odot \\sigma'(z^L).\n",
    "\\tag{30}\\end{eqnarray}$\n",
    "\n",
    "### Equation 2: error $\\delta^l$ in terms of the error in the next layer, $\\delta^l+1$\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\n",
    "\\tag{BP2}\\end{eqnarray}$\n",
    "\n",
    "Intuition: \n",
    "- move the error $\\delta^{l+1}$ backward through the network by appling the transpose weight matrix $(w^{l+1})^T$\n",
    "- $\\odot \\sigma'(z^l)$ Hadamard product  moves the error backward through the activation function in layer\n",
    "\n",
    "### Equation 3: rate of change of the cost with respect to any bias in the network\n",
    "\n",
    "$\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\n",
    "  \\delta^l_j\n",
    "\\tag{BP3}\\end{eqnarray}$\n",
    "\n",
    "Apply (BP1) and (BP2)\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial b} = \\delta\n",
    "\\tag{31}\\end{eqnarray}$\n",
    "\n",
    "$\\delta$ is being evaluated at the same neuron as the bias b\n",
    "\n",
    "### Equation 4: rate of change of the cost with respect to any weight in the network\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\n",
    "\\tag{BP4}\\end{eqnarray}$\n",
    "\n",
    "Equivalent to:\n",
    "\n",
    "$\\begin{eqnarray}  \\frac{\\partial\n",
    "    C}{\\partial w} = a_{\\rm in} \\delta_{\\rm out},\n",
    "\\tag{32}\\end{eqnarray}$\n",
    "\n",
    "\n",
    "weight learn slowly when $ \\partial C / \\partial \\approx 0w $\n",
    "- $a_{\\rm in} \\approx 0$\n",
    "- $\\sigma'(z^L_j) \\approx 0$ when $\\sigma(z^L_j) \\approx 0 $ (low activation) or $\\sigma(z^L_j) \\approx 1 $ (high activation)\n",
    "- the output neuron has saturated andthe weight has stopped learning (or is learning slowly)\n",
    "\n",
    "Long example of slow learning <http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function>\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "<http://neuralnetworksanddeeplearning.com/chap2.html#the_backpropagation_algorithm>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
